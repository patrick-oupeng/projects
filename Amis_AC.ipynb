{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cd3f906-9f9d-47ff-bb9a-dec00cc335ae",
   "metadata": {},
   "source": [
    "In this notebook I am going to try and use the [Formosanbank](https://ai4commsci.gitbook.io/formosanbank/the-bank-architecture/corpora) resources to make a basic autocorrect prediction system for Amis.\n",
    "Even assuming the corpora have enough word distribution to be useful for a frequency-based prediction system, I already see the following problems:\n",
    "* I don't know any Amis, and will have difficulty checking correctness\n",
    "* No existing test sets\n",
    "* Although I'm not sure how similar the morphology of Amis and Truku are (probably not very), I have no idea how an autocorrect would work for a fusional (?) language. If 'blah' is go and 'klah' is went and 'glah' is will go, should there be some bias toward forms of this stem if the input is similar enough in certain ways? At that point it starts to bleed into a stemmer/tagger.\n",
    "\n",
    "**Note**: I have already used some of the validation/sterilization tools in FormosanBank. According to the FormosanBank README it should have been a few simple checks and changes like standardizing all punctuation to single-spaced equivalents, but I am unclear what exactly was changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c0a19ef-d97a-4eb0-a257-6a41815dd0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_dir = \"../FormosanBank/Corpora\"\n",
    "find_lang = \"ami\" # Amis\n",
    "find_glotto = \"cent2104\"\n",
    "find_dialect = \"Coastal\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685bc337-a894-4171-b8ae-3300cdec39bf",
   "metadata": {},
   "source": [
    "First, let's make sure we're finding the expected files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b14f16d0-bcbd-4667-a7dd-963f22c41b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17108\n",
      "['../FormosanBank/Corpora/Wikipedias/XML/Seediq/Hnigan.xml', '../FormosanBank/Corpora/Wikipedias/XML/Seediq/Jingay_siang.xml']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "all_xmls = []\n",
    "for root, dirname, filenames in os.walk(corpora_dir):\n",
    "    for f in filenames:\n",
    "        if f.endswith(\"xml\"):\n",
    "            all_xmls.append(os.path.join(root,f))\n",
    "print(len(all_xmls))\n",
    "print(all_xmls[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9594d2d-b210-4acc-af43-89f3d331f8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glotto: None | dialect: None | file: Corpora Presidential_Apologies XML Amis Amis.xml\n",
      "glotto: None | dialect: None | file: Corpora ILRDF_Dicts XML Amis Amis.xml\n",
      "glotto: None | dialect: None | file: FormosanBank Corpora Virginia_Fey_Dictionary XML Amis.xml\n"
     ]
    }
   ],
   "source": [
    "lang_xmls = []\n",
    "import xml.etree.ElementTree as ET\n",
    "for filepath in all_xmls:\n",
    "    tree = ET.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "    # taken from formosanbank validate_xml.py\n",
    "    lang = root.get(\"{http://www.w3.org/XML/1998/namespace}lang\")\n",
    "    if lang:\n",
    "        lang = lang.lower()\n",
    "    else:\n",
    "        # print(f\"{filepath} doesn't appear to have a [lang] attrib: {root.attrib}\")\n",
    "        continue\n",
    "    glottocode = root.get(\"glottocode\")\n",
    "    dialect = root.get(\"dialect\")\n",
    "    if lang.lower() == find_lang.lower():\n",
    "        if not glottocode and not dialect: # they're both None\n",
    "            print(f\"glotto: {glottocode} | dialect: {dialect} | file: {' '.join(filepath.split('/')[-5:])}\")\n",
    "            # we assume the language is correct\n",
    "            lang_xmls.append(filepath)\n",
    "        else:\n",
    "            if glottocode:\n",
    "                if glottocode.lower() == find_glotto:\n",
    "                    lang_xmls.append(filepath)\n",
    "            if dialect:\n",
    "                if glottocode.lower() == find_dialect:\n",
    "                    lang_xmls.append(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a0c6eb3-1f96-44ed-bea6-608f16b7bdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "print(len(lang_xmls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b46d77-d700-4895-989e-794a0cd6d512",
   "metadata": {},
   "source": [
    "Now it looks like we have the right files for the language and dialect we want. There is gray area here in terms of langauge and dialect definition, but this should be good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe42523d-370e-483f-9c91-a41354327382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_list(root) -> list[str]:\n",
    "    sents = root.findall(\".//S\")\n",
    "    texts = []\n",
    "    for s in sents:\n",
    "        form_children = []\n",
    "        for child in s:\n",
    "            if child.tag == \"FORM\":\n",
    "                form_children.append(child)\n",
    "            # there is 'standard' and 'original' forms\n",
    "            if len(form_children) == 1:\n",
    "                texts.append(form_children[0].text)\n",
    "            else:\n",
    "                for child in form_children:\n",
    "                    kind = child.get(\"kindOf\")\n",
    "                    if kind == \"standard\":\n",
    "                        texts.append(child.text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1caa662-7db3-4ee8-a427-45e06297b5e1",
   "metadata": {},
   "source": [
    "Let's test our new method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60785455-7bb2-46ee-a7e7-73b3e02a36b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../FormosanBank/Corpora/ePark/XML/ep3_文化篇/Amis/Coastal_Amis.xml\n",
      "2128\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "testfile = lang_xmls[0]\n",
    "print(testfile)\n",
    "root = ET.parse(testfile).getroot()\n",
    "ret = get_sent_list(root)\n",
    "print(len(ret))\n",
    "print(root.get(\"doesnotexist\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9a8fea-0a2d-473b-90a0-3db27f27384c",
   "metadata": {},
   "source": [
    "Great, now let's see how it works on our collected files. Hopefully it's all formatted correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26b2b64a-336b-4176-8115-ecbee719cc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep3_文化篇 Amis Coastal_Amis.xml length: 2128\n",
      "ep2_文化篇 Amis Coastal_Amis.xml length: 2128\n",
      "ep2_閱讀書寫篇 Amis Coastal_Amis.xml length: 3375\n",
      "ep3_族語短文 Amis Coastal_Amis.xml length: 492\n",
      "ep3_繪本平台 Amis Coastal_Amis.xml length: 5028\n",
      "ep2_族語短文 Amis Coastal_Amis.xml length: 814\n",
      "ep2_生活會話篇 Amis Coastal_Amis.xml length: 3096\n",
      "ep3_圖畫故事篇 Amis Coastal_Amis.xml length: 172\n",
      "ep3_句型篇國中 Amis Coastal_Amis.xml length: 2404\n",
      "ep3_句型篇高中 Amis Coastal_Amis.xml length: 3456\n",
      "ep2_情境族語 Amis Coastal_Amis.xml length: 4578\n",
      "ep3_閱讀書寫篇 Amis Coastal_Amis.xml length: 3465\n",
      "ep3_情境族語 Amis Coastal_Amis.xml length: 3084\n",
      "ep3_生活會話篇 Amis Coastal_Amis.xml length: 3186\n",
      "ep2_學習詞表 Amis Coastal_Amis.xml length: 5444\n",
      "ep1_九階教材 Amis Coastal_Amis.xml length: 4618\n",
      "XML Amis Amis.xml length: 132\n",
      "XML Amis Amis.xml length: 21928\n",
      "Virginia_Fey_Dictionary XML Amis.xml length: 8883\n"
     ]
    }
   ],
   "source": [
    "all_sents = []\n",
    "for file in lang_xmls:\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "    file_text_as_list = get_sent_list(root)\n",
    "    print(f\"{' '.join(file.split('/')[-3:])} length: {len(file_text_as_list)}\")\n",
    "    all_sents += file_text_as_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d87953-63d1-4b11-9fd7-897c69c9353e",
   "metadata": {},
   "source": [
    "Hmm, it's a bit odd that the first two match exactly in length, and even though they're episode 2/3, they are also the same topic. I bet they're the same file accidentally - let's use a `set` to make sure we're not getting biased counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a07fa43-aede-4433-a11d-c842fb1bbe00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78411\n",
      "14103\n"
     ]
    }
   ],
   "source": [
    "print(len(all_sents))\n",
    "print(len(set(all_sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd12aaf5-2596-4376-a76b-4258c9488f6f",
   "metadata": {},
   "source": [
    "Whoa, that's a huge difference! There's a good chance that the Virginia Fey and others had single-word dictionaries and vocabularies that got duplicated. Let's check and see what's actually getting duplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00e378f3-6737-4b96-b0f7-5fff012ac5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts= {}\n",
    "for e in all_sents:\n",
    "    if e in counts:\n",
    "        counts[e] = counts[e] + 1\n",
    "    else:\n",
    "        counts[e] = 1\n",
    "i = sorted(counts.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5af14455-9cc4-46fe-b5b8-bbc7eed3243b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Cima kiso?', 51), ('masadak', 41), ('Cima ko ngangan iso?', 36), ('kaka', 34), (\"romi'ad\", 34), (\"mafana'\", 34), ('Papina ko salikaka iso?', 34), ('romadiw', 33), ('katayni', 33), (\"Nga'ay ho kiso?\", 32), ('adada', 30), ('tatodong', 29), ('folad', 29), ('anini', 29), ('kolong', 29), ('niyam', 29), (\"riko'\", 29), ('pising', 29), ('Talacowa kiso?', 29), ('masakero', 28)]\n"
     ]
    }
   ],
   "source": [
    "print(i[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27929d0c-e8e1-4eea-9fa2-570c78af7a9a",
   "metadata": {},
   "source": [
    "As expected, most are single words, but there are some sentences in there. This is a bit odd, but makes sense when you consider that most of our corpus is from an online language teaching platform. More than likely, a sentence is repeated multiple times throughout a 'teaching unit', and potentially across multiple units as well.\n",
    "\n",
    "Regardless, we now have a set of sentences - our corpus! - and can go ahead with normal Autocorrect stuff!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fd5ea37-f6cd-4a66-afe1-a6beb6c19c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_sentence_list= list(set(all_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a5c6007-ebb4-4676-b663-1fe43beb8a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for s in small_sentence_list:\n",
    "    corpus += [w.strip('/\\\\ .\",?!;:[]()<>#$%^&*-') for w in s.lower().split(' ') if w.strip(' \\'/\\\\ -.,\"?!;:[]()<>#$%^&*') != '']\n",
    "dictionary = set(corpus) # \"Dictionary\" of known words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c372a0d8-30e9-4417-b3e9-e130929162eb",
   "metadata": {},
   "source": [
    "I'm not so confident about putting everything to lowercase, but this should work. I'm unclear if I should be stripping the single quote `'` since some languages use it to demarcate a glottal stop.\n",
    "Anyway, at this point we have our corpus, and we have our set of all words that we've seen. Next we need to make a count for each word in the corpus, which will be used to predict which 'autocorrected' word is most likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99cbc349-3306-4c5f-8ee9-9956900bf8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcount = {}\n",
    "for w in corpus:\n",
    "    if w in wcount:\n",
    "        wcount[w] +=1\n",
    "    else:\n",
    "        wcount[w] = 1\n",
    "m = sum(wcount.values())\n",
    "wprobs = {k:wcount[k]/m for k in wcount} # probability of each word according to our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3fc6acdb-5261-4049-a616-caa8e9f6b498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus has 108582 words\n",
      "Sanity check for full corpus: True\n",
      "Corpus has 13340 unique words\n",
      "Sanity check for unique words: True\n",
      "Testing words: ['dahecong', \"ma'opiray\", 'a', 'pitaw', \"'alomaloman\", 'a', 'o', 'no']\n",
      "\n",
      "Count for dahecong: 1\n",
      "Prob  for dahecong: 9.209629588697943e-06\n",
      "\n",
      "Count for ma'opiray: 1\n",
      "Prob  for ma'opiray: 9.209629588697943e-06\n",
      "\n",
      "Count for a: 6336\n",
      "Prob  for a: 0.05835221307399016\n",
      "\n",
      "Count for pitaw: 11\n",
      "Prob  for pitaw: 0.00010130592547567737\n",
      "\n",
      "Count for 'alomaloman: 5\n",
      "Prob  for 'alomaloman: 4.604814794348971e-05\n",
      "\n",
      "Count for a: 6336\n",
      "Prob  for a: 0.05835221307399016\n",
      "\n",
      "Count for o: 3189\n",
      "Prob  for o: 0.02936950875835774\n",
      "\n",
      "Count for no: 5135\n",
      "Prob  for no: 0.047291447937963936\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Corpus has {len(corpus)} words\")\n",
    "print(f\"Sanity check for full corpus: {str(len(corpus) == m)}\")\n",
    "print(f\"Corpus has {len(dictionary)} unique words\")\n",
    "print(f\"Sanity check for unique words: {str(len(wcount) == len(wprobs) == len(dictionary))}\")\n",
    "temp_print_list = list(dictionary)[:5]\n",
    "temp_print_list += ['a', 'o', 'no'] # known common words\n",
    "print(f\"Testing words: {temp_print_list}\")\n",
    "print(\"\")\n",
    "for t in temp_print_list:\n",
    "    print(f\"Count for {t}: {wcount[t]}\")\n",
    "    print(f\"Prob  for {t}: {wprobs[t]}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17056288-daab-4427-8e1c-317ed0ce0e56",
   "metadata": {},
   "source": [
    "The above looks good, although I am a little bit suspicious of how high the counts are for `a` `no` and `o`. I suppose with over 100k words in the corpus this isn't that high.\n",
    "Now we will actually do autocorrect prediction. First, let's make a list of 'test' words - I'll make up a few to test based on real words from our dictionary. This would be a good thing to automate, where I find N words of various lengths and mangle them based on insertion, deletion, and swap, but for now I'll manually write the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "da02e32b-8dfc-4e68-b611-ee0951ed96bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key: input word to predict\n",
    "# value: expected/hoped for output\n",
    "words_to_test = {\n",
    "    'midalof': 'misalof', # s->d\n",
    "    'caykanac': 'caykanca', # missing -a\n",
    "    'inra': 'nira', # swap n-i \n",
    "    'kici': 'kicic', # missing -c \n",
    "    'noo': 'no', # additional o \n",
    "    'asdfjkal': 'UNK', # keyboard mash \n",
    "    \"'atongol\": \"'atongol\", # same - make sure we include the original words if they exist\n",
    "    \"kasarramoramod\": \"kasaramoramod\", # additional r\n",
    "    \"niyaro\": \"niyaro'\", # missing '\n",
    "    'x': 'a', # single-letter replacement; most likely should alwasy be 'a' or 'o'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2da61c12-b4a4-4de8-9648-ce95a0db085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.metrics.distance import edit_distance, jaccard_distance\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "254a4b01-1d5a-4d26-92b5-de62a127194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in an input to correct, and a metric to calculate distance to word in our dictionary\n",
    "# Returns a sorted dict of candidate words with the lowest distance\n",
    "def suggest(istr, metric):\n",
    "    candlist = {} # key: candidate word; value: distance\n",
    "    for w in dictionary:\n",
    "        # if w[0] == istr[0]:\n",
    "            if w == istr:\n",
    "                candlist[istr] = 0\n",
    "            if metric == 'jaccard':\n",
    "                # can't compute bigrams for single letter words\n",
    "                if len(istr) == 1:\n",
    "                    continue\n",
    "                # 2-char bigrams of our strings\n",
    "                i2grams = set(ngrams(istr,2))\n",
    "                w2grams = set(ngrams(w,2))\n",
    "                # calculate the distance between the two sets\n",
    "                dist = jaccard_distance(i2grams, w2grams)\n",
    "                # words are unique, distance may not be\n",
    "            elif metric == 'edit': # metric == 'edit'\n",
    "                dist = edit_distance(istr, w)\n",
    "            # below doesn't work - division is too powerful for small probabilities\n",
    "            elif metric == 'prob':\n",
    "                dist = probs[w]/edit_distance(istr,w)\n",
    "            else: # we didn't pass in a metric, oops\n",
    "                raise ValueError(\"You forgot to pass in a metric!\")\n",
    "            candlist[w] = dist\n",
    "    sorted_dict = {k:candlist[k] for k in sorted(candlist, key=candlist.get)}\n",
    "    return sorted_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9a22e0-d55d-4fc1-9094-4d348789a3f6",
   "metadata": {},
   "source": [
    "The above is a bit hacky, but right now I'm more interested in comparing the performance of the different metrics on our dataset. Ideally there would be a combination of multiple metrics, and statistical prediction of what the next word is likely to be based on word-level 2-grams (or n-grams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "88b07f31-879d-4be1-86e4-803a20a965ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing midalof, expected misalof\n",
      "\tFound: True\n",
      "\tjaccard: ['misalof', 'cidal', 'alofo', 'alomi', 'falofalo', 'lalosidan', 'midadiwal', 'tadalosid', 'midawa', 'milalo']\n",
      "\tFound: True\n",
      "\tjprob: {'cidal': 0.0008196570333941169, 'lalosidan': 0.000322337035604428, 'misalof': 5.525777753218766e-05, 'tadalosid': 2.762888876609383e-05, 'midawa': 1.8419259177395885e-05, 'alofo': 9.209629588697943e-06, 'alomi': 9.209629588697943e-06, 'falofalo': 9.209629588697943e-06, 'midadiwal': 9.209629588697943e-06, 'milalo': 9.209629588697943e-06}\n",
      "\tFound: True\n",
      "\tedit: ['misalof', 'masalof', 'milalo', 'midadoy', 'mipalo', 'mifalod', \"midafo'\", 'mitalod', 'midaho', 'mifalah']\n",
      "\tFound: True\n",
      "\teprob: {'misalof': 5.525777753218766e-05, 'mipalo': 3.683851835479177e-05, 'midaho': 3.683851835479177e-05, 'mifalah': 3.683851835479177e-05, 'masalof': 2.762888876609383e-05, 'milalo': 9.209629588697943e-06, 'midadoy': 9.209629588697943e-06, 'mifalod': 9.209629588697943e-06, \"midafo'\": 9.209629588697943e-06, 'mitalod': 9.209629588697943e-06}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Testing caykanac, expected caykanca\n",
      "\tFound: True\n",
      "\tjaccard: ['caykanca', 'caykangca', 'caayka', 'mananiwacay', 'caykaeca', 'kawananay', 'kinacay', 'kacawan', 'pacakay', 'kaca']\n",
      "\tFound: True\n",
      "\tjprob: {'caykanca': 0.00018419259177395884, 'pacakay': 5.525777753218766e-05, 'caykaeca': 3.683851835479177e-05, 'kacawan': 2.762888876609383e-05, 'caykangca': 9.209629588697943e-06, 'caayka': 9.209629588697943e-06, 'mananiwacay': 9.209629588697943e-06, 'kawananay': 9.209629588697943e-06, 'kinacay': 9.209629588697943e-06, 'kaca': 9.209629588697943e-06}\n",
      "\tFound: True\n",
      "\tedit: ['caykanca', 'caykangca', 'caykaeca', 'taywanay', 'cayna', 'makonac', 'caykeng', 'takaraw', 'oyanan', 'kakapah']\n",
      "\tFound: True\n",
      "\teprob: {'takaraw': 0.0004144333314914074, 'caykanca': 0.00018419259177395884, 'taywanay': 4.604814794348971e-05, 'caykaeca': 3.683851835479177e-05, 'oyanan': 3.683851835479177e-05, 'caykeng': 1.8419259177395885e-05, 'caykangca': 9.209629588697943e-06, 'cayna': 9.209629588697943e-06, 'makonac': 9.209629588697943e-06, 'kakapah': 9.209629588697943e-06}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Testing inra, expected nira\n",
      "\tFound: False\n",
      "\tjaccard: ['ningra', 'cingra', 'kinaira', 'cingra！', 'mingiraw', 'sinasera', 'yra', 'rarar', 'cingraan', 'cingranan']\n",
      "\tFound: False\n",
      "\tjprob: {'cingra': 0.005571825901162255, 'ningra': 0.000967011106813284, 'cingraan': 0.000478900738612293, 'kinaira': 0.00011051555506437532, 'sinasera': 8.288666629828149e-05, 'cingranan': 7.367703670958354e-05, 'yra': 2.762888876609383e-05, 'cingra！': 9.209629588697943e-06, 'mingiraw': 9.209629588697943e-06, 'rarar': 9.209629588697943e-06}\n",
      "\tFound: False\n",
      "\tedit: ['ira', 'ina', 'iro', 'nina', 'tora', 'kira', 'ika', 'ngra', 'ciira', 'yra']\n",
      "\tFound: False\n",
      "\teprob: {'ira': 0.005065296273783868, 'ina': 0.0034351918365843324, 'kira': 0.002523438507303236, 'ciira': 0.0014827503637803688, 'tora': 0.0004604814794348971, 'nina': 0.00013814444383046913, 'iro': 4.604814794348971e-05, 'ika': 3.683851835479177e-05, 'yra': 2.762888876609383e-05, 'ngra': 1.8419259177395885e-05}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Testing kici, expected kicic\n",
      "\tFound: True\n",
      "\tjaccard: ['kicic', 'fakici', 'sicikil', 'cicir', 'cifakici', 'ciciw', 'cicih', 'fakiciay', 'kiric', 'acicim']\n",
      "\tFound: True\n",
      "\tjprob: {'ciciw': 0.00021182148054005269, 'fakici': 8.288666629828149e-05, 'kicic': 2.762888876609383e-05, 'fakiciay': 1.8419259177395885e-05, 'sicikil': 9.209629588697943e-06, 'cicir': 9.209629588697943e-06, 'cifakici': 9.209629588697943e-06, 'cicih': 9.209629588697943e-06, 'kiric': 9.209629588697943e-06, 'acicim': 9.209629588697943e-06}\n",
      "\tFound: True\n",
      "\tedit: ['kaci', 'kini', 'kicic', 'kipin', 'dipi', 'kopi', 'tini', 'ko-ci', 'kira', 'kima']\n",
      "\tFound: True\n",
      "\teprob: {'kira': 0.002523438507303236, 'tini': 0.0002854985172496362, 'kini': 3.683851835479177e-05, 'kipin': 3.683851835479177e-05, 'kopi': 3.683851835479177e-05, 'kicic': 2.762888876609383e-05, 'kaci': 1.8419259177395885e-05, 'dipi': 1.8419259177395885e-05, 'ko-ci': 9.209629588697943e-06, 'kima': 9.209629588697943e-06}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Testing noo, expected no\n",
      "\tFound: True\n",
      "\tjaccard: ['no', 'kanoos', 'tenoon', 'ano', 'ono', 'ino', 'pitenooy', 'tanoolah', 'mitenoon', 'cikanoos']\n",
      "\tFound: True\n",
      "\tjprob: {'no': 0.047291447937963936, 'ano': 0.0036101747987695933, 'pitenooy': 0.00011051555506437532, 'ono': 5.525777753218766e-05, 'mitenoon': 3.683851835479177e-05, 'kanoos': 1.8419259177395885e-05, 'tenoon': 1.8419259177395885e-05, 'ino': 9.209629588697943e-06, 'tanoolah': 9.209629588697943e-06, 'cikanoos': 9.209629588697943e-06}\n",
      "\tFound: True\n",
      "\tedit: ['nao', 'nto', 'no', 'roto', 'fo', 'hok', \"'o'o\", 'iro', 'so', 'nano']\n",
      "\tFound: True\n",
      "\teprob: {'no': 0.047291447937963936, 'nano': 0.0007551896262732313, 'fo': 4.604814794348971e-05, 'iro': 4.604814794348971e-05, 'nao': 1.8419259177395885e-05, 'hok': 1.8419259177395885e-05, 'nto': 9.209629588697943e-06, 'roto': 9.209629588697943e-06, \"'o'o\": 9.209629588697943e-06, 'so': 9.209629588697943e-06}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Testing asdfjkal, expected UNK\n",
      "\tFound: False\n",
      "\tjaccard: ['kalas', 'kalasak', 'kalasaka', 'kal', 'talakasi', 'kasasowal', 'kasaselal', 'malasaka', 'pakalasen', 'kasomowal']\n",
      "\tFound: False\n",
      "\tjprob: {'malasaka': 0.00031312740601573006, 'kasaselal': 0.00012893481424177118, 'kasomowal': 6.446740712088559e-05, 'kalas': 4.604814794348971e-05, 'kalasak': 9.209629588697943e-06, 'kalasaka': 9.209629588697943e-06, 'kal': 9.209629588697943e-06, 'talakasi': 9.209629588697943e-06, 'kasasowal': 9.209629588697943e-06, 'pakalasen': 9.209629588697943e-06}\n",
      "\tFound: False\n",
      "\tedit: ['sikal', 'sofal', 'alekol', 'safel', 'malifokay', 'masasowal', 'masamolal', \"sifana'\", 'aday^ay', 'patalakal']\n",
      "\tFound: False\n",
      "\teprob: {\"sifana'\": 0.0001749829621852609, 'masasowal': 7.367703670958354e-05, 'sofal': 5.525777753218766e-05, 'sikal': 2.762888876609383e-05, 'safel': 2.762888876609383e-05, 'masamolal': 2.762888876609383e-05, 'malifokay': 1.8419259177395885e-05, 'alekol': 9.209629588697943e-06, 'aday^ay': 9.209629588697943e-06, 'patalakal': 9.209629588697943e-06}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Testing 'atongol, expected 'atongol\n",
      "\tFound: True\n",
      "\tjaccard: [\"'atongol\", \"'atatongol\", 'atong', \"'atol\", 'katangongol', 'datong', 'matatongod', 'tongod', 'patongoden', 'matatongotongod']\n",
      "\tFound: True\n",
      "\tjprob: {'atong': 7.367703670958354e-05, \"'atatongol\": 1.8419259177395885e-05, \"'atol\": 1.8419259177395885e-05, 'katangongol': 1.8419259177395885e-05, 'datong': 1.8419259177395885e-05, \"'atongol\": 9.209629588697943e-06, 'matatongod': 9.209629588697943e-06, 'tongod': 9.209629588697943e-06, 'patongoden': 9.209629588697943e-06, 'matatongotongod': 9.209629588697943e-06}\n",
      "\tFound: True\n",
      "\tedit: [\"'atongol\", 'matongal', \"'atatongol\", 'patongal', 'atong', 'matengil', 'tatinol', 'tangongol', \"'akong\", 'takonol']\n",
      "\tFound: True\n",
      "\teprob: {'matengil': 0.00014735407341916708, 'matongal': 0.00013814444383046913, 'atong': 7.367703670958354e-05, \"'atatongol\": 1.8419259177395885e-05, 'tatinol': 1.8419259177395885e-05, 'tangongol': 1.8419259177395885e-05, \"'akong\": 1.8419259177395885e-05, 'takonol': 1.8419259177395885e-05, \"'atongol\": 9.209629588697943e-06, 'patongal': 9.209629588697943e-06}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Testing kasarramoramod, expected kasaramoramod\n",
      "\tFound: True\n",
      "\tjaccard: ['kasaramoramod', 'kararamod', 'sakamoraraw', 'kamoraraw', 'kasasorar', 'kalaramod', 'raramod', 'kasamo', 'pararamod', 'paramod']\n",
      "\tFound: True\n",
      "\tjprob: {'kararamod': 4.604814794348971e-05, 'kasasorar': 1.8419259177395885e-05, 'paramod': 1.8419259177395885e-05, 'kasaramoramod': 9.209629588697943e-06, 'sakamoraraw': 9.209629588697943e-06, 'kamoraraw': 9.209629588697943e-06, 'kalaramod': 9.209629588697943e-06, 'raramod': 9.209629588697943e-06, 'kasamo': 9.209629588697943e-06, 'pararamod': 9.209629588697943e-06}\n",
      "\tFound: True\n",
      "\tedit: ['kasaramoramod', 'kararamod', 'kasarayray', 'pararamod', 'kasarayaray', 'kalaramod', 'kasarorok', 'kasaparod', \"kasa'alo'alo\", 'mararamod']\n",
      "\tFound: True\n",
      "\teprob: {'kararamod': 4.604814794348971e-05, 'mararamod': 3.683851835479177e-05, 'kasarayray': 1.8419259177395885e-05, 'kasarayaray': 1.8419259177395885e-05, 'kasaramoramod': 9.209629588697943e-06, 'pararamod': 9.209629588697943e-06, 'kalaramod': 9.209629588697943e-06, 'kasarorok': 9.209629588697943e-06, 'kasaparod': 9.209629588697943e-06, \"kasa'alo'alo\": 9.209629588697943e-06}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Testing niyaro, expected niyaro'\n",
      "\tFound: True\n",
      "\tjaccard: ['niyaro', \"niyaro'\", \"niyaro'an\", \"paniyaro'\", \"niyaro'ay\", 'niriyar', \"paniyaro'an\", \"talaniyaro'\", \"kasaniyaro'\", 'riyar']\n",
      "\tFound: True\n",
      "\tjprob: {\"niyaro'\": 0.0029470814683833415, 'riyar': 0.0013630251791272956, 'niyaro': 0.00016577333259656298, \"paniyaro'an\": 0.00016577333259656298, \"kasaniyaro'\": 0.00014735407341916708, \"talaniyaro'\": 9.209629588697942e-05, \"paniyaro'\": 5.525777753218766e-05, \"niyaro'ay\": 5.525777753218766e-05, \"niyaro'an\": 9.209629588697943e-06, 'niriyar': 9.209629588697943e-06}\n",
      "\tFound: True\n",
      "\tedit: ['niyaro', \"niyaro'\", 'riyar', 'nikar', 'niyam', 'niara', 'pinaro', 'niyah', 'piparo', 'miparo']\n",
      "\tFound: True\n",
      "\teprob: {'niyam': 0.003950931093551417, \"niyaro'\": 0.0029470814683833415, 'riyar': 0.0013630251791272956, 'niyaro': 0.00016577333259656298, 'niyah': 0.00011972518465307325, 'miparo': 8.288666629828149e-05, 'nikar': 1.8419259177395885e-05, 'pinaro': 1.8419259177395885e-05, 'niara': 9.209629588697943e-06, 'piparo': 9.209629588697943e-06}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Testing x, expected a\n",
      "\tFound: False\n",
      "\tjaccard: []\n",
      "\tFound: False\n",
      "\tjprob: {}\n",
      "\tFound: True\n",
      "\tedit: ['a', '4', 's', '3', '7', 'o', '5', '9', 'i', '8']\n",
      "\tFound: True\n",
      "\teprob: {'a': 0.05835221307399016, 'i': 0.030097069495864878, 'o': 0.02936950875835774, '5': 5.525777753218766e-05, '3': 3.683851835479177e-05, '9': 3.683851835479177e-05, '8': 3.683851835479177e-05, '7': 2.762888876609383e-05, '4': 1.8419259177395885e-05, 's': 1.8419259177395885e-05}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for testword in words_to_test:\n",
    "    expected = words_to_test[testword]\n",
    "    # get the top X words according to our metric\n",
    "    # having a threshold would be better, like n steps away according to edit distance etc.\n",
    "    get_x_words = 10\n",
    "\n",
    "    # get words suggested by jaccard metric\n",
    "    jsug = list(suggest(testword, 'jaccard'))[:get_x_words] \n",
    "    jprobs = {w: wprobs[w] for w in jsug}\n",
    "    # sort jaccard words by probability\n",
    "    jprobs_sorted = {k: jprobs[k] for k in sorted(jprobs, key = jprobs.get, reverse=True)}\n",
    "    \n",
    "    # do the same for edit distance\n",
    "    esug = list(suggest(testword, 'edit'))[:get_x_words]\n",
    "    eprobs = {w: wprobs[w] for w in esug}\n",
    "    eprobs_sorted = {k: eprobs[k] for k in sorted(eprobs, key = eprobs.get, reverse=True)}\n",
    "\n",
    "    print(f\"Testing {testword}, expected {expected}\")\n",
    "    print(\"\\t\" + f\"Found: {str(expected in jsug)}\" + \"\\n\\t\" + f\"jaccard: {str(jsug)}\")\n",
    "    print(\"\\t\" + f\"Found: {str(expected in jprobs_sorted)}\" + \"\\n\\t\" + f\"jprob: {str(jprobs_sorted)}\")\n",
    "    print(\"\\t\" + f\"Found: {str(expected in esug)}\" + \"\\n\\t\" + f\"edit: {str(esug)}\")\n",
    "    print(\"\\t\" + f\"Found: {str(expected in eprobs_sorted)}\" + \"\\n\\t\" + f\"eprob: {str(eprobs_sorted)}\")\n",
    "    print(\"\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9088634a-5481-41fd-8b49-ca03bb63e433",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "From the above, it appears that it works okay for now, although there are a few issues. For now these are my thoughts.\n",
    "1. Jaccard fails for single-letter words. (It's fixed now, but I had to go back and add a check to skip Jaccard for single letters.) This is expected, but good to know.\n",
    "2. Shorter words have difficulty when the error is a swap. This is because for jaccard distance there are too few bigrams to do meaningful comparison when the word is short; for edit distance the swap is counted as 2 moves (I think), and short words will be equidistant from many other words by virtue of being a potential substring.\n",
    "3. It is useful to sort the candidate words based on probability. Ideally there should be some sort of computation based on distance, probability, and even word prediction.\n",
    "4. I missed some full-width punctuation in my string sterilization, as well as a few symbols (I've fixed some now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4cbcd3-1ae6-4170-b066-6fbf85910616",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
