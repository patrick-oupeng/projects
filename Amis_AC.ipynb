{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cd3f906-9f9d-47ff-bb9a-dec00cc335ae",
   "metadata": {},
   "source": [
    "In this notebook I am going to try and use the [Formosanbank](https://ai4commsci.gitbook.io/formosanbank/the-bank-architecture/corpora) resources to make a basic autocorrect prediction system for Amis.\n",
    "Even assuming the corpora have enough word distribution to be useful for a frequency-based prediction system, I already see the following problems:\n",
    "* I don't know any Amis, and will have difficulty checking correctness\n",
    "* No existing test sets\n",
    "* Although I'm not sure how similar the morphology of Amis and Truku are (probably not very), I have no idea how an autocorrect would work for a fusional (?) language. If 'blah' is go and 'klah' is went and 'glah' is will go, should there be some bias toward forms of this stem if the input is similar enough in certain ways? At that point it starts to bleed into a stemmer/tagger.\n",
    "\n",
    "**Note**: I have already used some of the validation/sterilization tools in FormosanBank. According to the FormosanBank README it should have been a few simple checks and changes like standardizing all punctuation to single-spaced equivalents, but I am unclear what exactly was changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c0a19ef-d97a-4eb0-a257-6a41815dd0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_dir = \"./FormosanBank/Corpora\"\n",
    "find_lang = \"ami\" # Amis\n",
    "find_glotto = \"cent2104\"\n",
    "find_dialect = \"Coastal\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685bc337-a894-4171-b8ae-3300cdec39bf",
   "metadata": {},
   "source": [
    "First, let's make sure we're finding the expected files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b14f16d0-bcbd-4667-a7dd-963f22c41b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17108\n",
      "['./FormosanBank/Corpora/Wikipedias/XML/Seediq/Hnigan.xml', './FormosanBank/Corpora/Wikipedias/XML/Seediq/Jingay_siang.xml']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "all_xmls = []\n",
    "for root, dirname, filenames in os.walk(corpora_dir):\n",
    "    for f in filenames:\n",
    "        if f.endswith(\"xml\"):\n",
    "            all_xmls.append(os.path.join(root,f))\n",
    "print(len(all_xmls))\n",
    "print(all_xmls[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9594d2d-b210-4acc-af43-89f3d331f8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glotto: None | dialect: None | file: Corpora Presidential_Apologies XML Amis Amis.xml\n",
      "glotto: None | dialect: None | file: Corpora ILRDF_Dicts XML Amis Amis.xml\n",
      "glotto: None | dialect: None | file: FormosanBank Corpora Virginia_Fey_Dictionary XML Amis.xml\n"
     ]
    }
   ],
   "source": [
    "lang_xmls = []\n",
    "import xml.etree.ElementTree as ET\n",
    "for filepath in all_xmls:\n",
    "    tree = ET.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "    # taken from formosanbank validate_xml.py\n",
    "    lang = root.get(\"{http://www.w3.org/XML/1998/namespace}lang\")\n",
    "    if lang:\n",
    "        lang = lang.lower()\n",
    "    else:\n",
    "        # print(f\"{filepath} doesn't appear to have a [lang] attrib: {root.attrib}\")\n",
    "        continue\n",
    "    glottocode = root.get(\"glottocode\")\n",
    "    dialect = root.get(\"dialect\")\n",
    "    if lang.lower() == find_lang.lower():\n",
    "        if not glottocode and not dialect: # they're both None\n",
    "            print(f\"glotto: {glottocode} | dialect: {dialect} | file: {' '.join(filepath.split('/')[-5:])}\")\n",
    "            # we assume the language is correct\n",
    "            lang_xmls.append(filepath)\n",
    "        else:\n",
    "            if glottocode:\n",
    "                if glottocode.lower() == find_glotto:\n",
    "                    lang_xmls.append(filepath)\n",
    "            if dialect:\n",
    "                if glottocode.lower() == find_dialect:\n",
    "                    lang_xmls.append(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a0c6eb3-1f96-44ed-bea6-608f16b7bdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "print(len(lang_xmls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b46d77-d700-4895-989e-794a0cd6d512",
   "metadata": {},
   "source": [
    "Now it looks like we have the right files for the language and dialect we want. There is gray area here in terms of langauge and dialect definition, but this should be good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe42523d-370e-483f-9c91-a41354327382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_list(root) -> list[str]:\n",
    "    sents = root.findall(\".//S\")\n",
    "    texts = []\n",
    "    for s in sents:\n",
    "        form_children = []\n",
    "        for child in s:\n",
    "            if child.tag == \"FORM\":\n",
    "                form_children.append(child)\n",
    "            # there is 'standard' and 'original' forms\n",
    "            if len(form_children) == 1:\n",
    "                texts.append(form_children[0].text)\n",
    "            else:\n",
    "                for child in form_children:\n",
    "                    kind = child.get(\"kindOf\")\n",
    "                    if kind == \"standard\":\n",
    "                        texts.append(child.text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1caa662-7db3-4ee8-a427-45e06297b5e1",
   "metadata": {},
   "source": [
    "Let's test our new method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60785455-7bb2-46ee-a7e7-73b3e02a36b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./FormosanBank/Corpora/ePark/XML/ep3_文化篇/Amis/Coastal_Amis.xml\n",
      "2128\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "testfile = lang_xmls[0]\n",
    "print(testfile)\n",
    "root = ET.parse(testfile).getroot()\n",
    "ret = get_sent_list(root)\n",
    "print(len(ret))\n",
    "print(root.get(\"doesnotexist\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9a8fea-0a2d-473b-90a0-3db27f27384c",
   "metadata": {},
   "source": [
    "Great, now let's see how it works on our collected files. Hopefully it's all formatted correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26b2b64a-336b-4176-8115-ecbee719cc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep3_文化篇 Amis Coastal_Amis.xml length: 2128\n",
      "ep2_文化篇 Amis Coastal_Amis.xml length: 2128\n",
      "ep2_閱讀書寫篇 Amis Coastal_Amis.xml length: 3375\n",
      "ep3_族語短文 Amis Coastal_Amis.xml length: 492\n",
      "ep3_繪本平台 Amis Coastal_Amis.xml length: 5028\n",
      "ep2_族語短文 Amis Coastal_Amis.xml length: 814\n",
      "ep2_生活會話篇 Amis Coastal_Amis.xml length: 3096\n",
      "ep3_圖畫故事篇 Amis Coastal_Amis.xml length: 172\n",
      "ep3_句型篇國中 Amis Coastal_Amis.xml length: 2404\n",
      "ep3_句型篇高中 Amis Coastal_Amis.xml length: 3456\n",
      "ep2_情境族語 Amis Coastal_Amis.xml length: 4578\n",
      "ep3_閱讀書寫篇 Amis Coastal_Amis.xml length: 3465\n",
      "ep3_情境族語 Amis Coastal_Amis.xml length: 3084\n",
      "ep3_生活會話篇 Amis Coastal_Amis.xml length: 3186\n",
      "ep2_學習詞表 Amis Coastal_Amis.xml length: 5444\n",
      "ep1_九階教材 Amis Coastal_Amis.xml length: 4618\n",
      "XML Amis Amis.xml length: 132\n",
      "XML Amis Amis.xml length: 21928\n",
      "Virginia_Fey_Dictionary XML Amis.xml length: 8883\n"
     ]
    }
   ],
   "source": [
    "all_sents = []\n",
    "for file in lang_xmls:\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "    file_text_as_list = get_sent_list(root)\n",
    "    print(f\"{' '.join(file.split('/')[-3:])} length: {len(file_text_as_list)}\")\n",
    "    all_sents += file_text_as_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d87953-63d1-4b11-9fd7-897c69c9353e",
   "metadata": {},
   "source": [
    "Hmm, it's a bit odd that the first two match exactly in length, and even though they're episode 2/3, they are also the same topic. I bet they're the same file accidentally - let's use a `set` to make sure we're not getting biased counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a07fa43-aede-4433-a11d-c842fb1bbe00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78411\n",
      "14103\n"
     ]
    }
   ],
   "source": [
    "print(len(all_sents))\n",
    "print(len(set(all_sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd12aaf5-2596-4376-a76b-4258c9488f6f",
   "metadata": {},
   "source": [
    "Whoa, that's a huge difference! There's a good chance that the Virginia Fey and others had single-word dictionaries and vocabularies that got duplicated. Let's check and see what's actually getting duplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00e378f3-6737-4b96-b0f7-5fff012ac5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts= {}\n",
    "for e in all_sents:\n",
    "    if e in counts:\n",
    "        counts[e] = counts[e] + 1\n",
    "    else:\n",
    "        counts[e] = 1\n",
    "i = sorted(counts.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5af14455-9cc4-46fe-b5b8-bbc7eed3243b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Cima kiso?', 51), ('masadak', 41), ('Cima ko ngangan iso?', 36), ('kaka', 34), (\"romi'ad\", 34), (\"mafana'\", 34), ('Papina ko salikaka iso?', 34), ('romadiw', 33), ('katayni', 33), (\"Nga'ay ho kiso?\", 32), ('adada', 30), ('tatodong', 29), ('folad', 29), ('anini', 29), ('kolong', 29), ('niyam', 29), (\"riko'\", 29), ('pising', 29), ('Talacowa kiso?', 29), ('masakero', 28)]\n"
     ]
    }
   ],
   "source": [
    "print(i[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27929d0c-e8e1-4eea-9fa2-570c78af7a9a",
   "metadata": {},
   "source": [
    "As expected, most are single words, but there are some sentences in there. This is a bit odd, but makes sense when you consider that most of our corpus is from an online language teaching platform. More than likely, a sentence is repeated multiple times throughout a 'unit', and potentially across multiple units as well.\n",
    "\n",
    "Regardless, we now have a set of sentences - our corpus! - and can go ahead with normal Autocorrect stuff!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fd5ea37-f6cd-4a66-afe1-a6beb6c19c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_sentence_list= list(set(all_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a5c6007-ebb4-4676-b663-1fe43beb8a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for s in small_sentence_list:\n",
    "    corpus += [w.strip('/\\\\ .,?!;:[]()<>#$%^&*') for w in s.lower().split(' ') if w.strip(' \\'/\\\\ .,?!;:[]()<>#$%^&*') != '']\n",
    "dictionary = set(corpus) # \"Dictionary\" of known words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c372a0d8-30e9-4417-b3e9-e130929162eb",
   "metadata": {},
   "source": [
    "I'm not so confident about putting everything to lowercase, but this should work. I'm unclear if I should be stripping the single quote `'` since some languages use it to demarcate a glottal stop.\n",
    "Anyway, at this point we have our corpus, and we have our set of all words that we've seen. Next we need to make a count for each word in the corpus, which will be used to predict which 'autocorrected' word is most likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99cbc349-3306-4c5f-8ee9-9956900bf8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcount = {}\n",
    "for w in corpus:\n",
    "    if w in wcount:\n",
    "        wcount[w] +=1\n",
    "    else:\n",
    "        wcount[w] = 1\n",
    "m = sum(wcount.values())\n",
    "wprobs = {k:wcount[k]/m for k in wcount} # probability of each word according to our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3fc6acdb-5261-4049-a616-caa8e9f6b498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus has 108919 words\n",
      "Sanity check for full corpus: True\n",
      "Corpus has 13695 unique words\n",
      "Sanity check for unique words: True\n",
      "Testing words: ['nipacamolan', 'masanekay', \"mianako'\", 'satadamaanay', \"misatata'ak\", 'hofocan', \"maloci'ay\", \"li'akong\", 'longec', \"i'ayawho\", 'a', 'o', 'no']\n",
      "\n",
      "Count for nipacamolan: 1\n",
      "Prob  for nipacamolan: 9.181134604614438e-06\n",
      "Count for masanekay: 2\n",
      "Prob  for masanekay: 1.8362269209228875e-05\n",
      "Count for mianako': 1\n",
      "Prob  for mianako': 9.181134604614438e-06\n",
      "Count for satadamaanay: 7\n",
      "Prob  for satadamaanay: 6.426794223230107e-05\n",
      "Count for misatata'ak: 1\n",
      "Prob  for misatata'ak: 9.181134604614438e-06\n",
      "Count for hofocan: 1\n",
      "Prob  for hofocan: 9.181134604614438e-06\n",
      "Count for maloci'ay: 1\n",
      "Prob  for maloci'ay: 9.181134604614438e-06\n",
      "Count for li'akong: 1\n",
      "Prob  for li'akong: 9.181134604614438e-06\n",
      "Count for longec: 1\n",
      "Prob  for longec: 9.181134604614438e-06\n",
      "Count for i'ayawho: 1\n",
      "Prob  for i'ayawho: 9.181134604614438e-06\n",
      "Count for a: 6334\n",
      "Prob  for a: 0.058153306585627854\n",
      "Count for o: 3145\n",
      "Prob  for o: 0.028874668331512407\n",
      "Count for no: 5132\n",
      "Prob  for no: 0.047117582790881296\n"
     ]
    }
   ],
   "source": [
    "print(f\"Corpus has {len(corpus)} words\")\n",
    "print(f\"Sanity check for full corpus: {str(len(corpus) == m)}\")\n",
    "print(f\"Corpus has {len(dictionary)} unique words\")\n",
    "print(f\"Sanity check for unique words: {str(len(wcount) == len(wprobs) == len(dictionary))}\")\n",
    "testwords = list(dictionary)[:10]\n",
    "testwords += ['a', 'o', 'no'] # known common words\n",
    "print(f\"Testing words: {testwords}\")\n",
    "print(\"\")\n",
    "for t in testwords:\n",
    "    print(f\"Count for {t}: {wcount[t]}\")\n",
    "    print(f\"Prob  for {t}: {wprobs[t]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17056288-daab-4427-8e1c-317ed0ce0e56",
   "metadata": {},
   "source": [
    "The above looks good, although I am a little bit suspicious of how high the counts are for `a` `no` and `o`. I suppose with over 100k words in the corpus this isn't that high.\n",
    "Now we will actually do autocorrect prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da02e32b-8dfc-4e68-b611-ee0951ed96bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
